{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c46cbff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\hmallam\\appdata\\local\\anaconda3\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\hmallam\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hmallam\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\hmallam\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\hmallam\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hmallam\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\hmallam\\appdata\\local\\anaconda3\\lib\\site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hmallam\\appdata\\local\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\hmallam\\appdata\\local\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ef7d8826",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################\n",
    "##### Linear regression using PyTorch built-ins ####\n",
    "#####################################################################################################\n",
    "\n",
    "# torch.nn package from PyTorch, which contains utility classes for building neural networks\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Input (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43], [91, 88, 64], [87, 134, 58], \n",
    "                   [102, 43, 37], [69, 96, 70], [73, 67, 43], \n",
    "                   [91, 88, 64], [87, 134, 58], [102, 43, 37], \n",
    "                   [69, 96, 70], [73, 67, 43], [91, 88, 64], \n",
    "                   [87, 134, 58], [102, 43, 37], [69, 96, 70]], \n",
    "                  dtype='float32')\n",
    "\n",
    "# Targets (apples, oranges)\n",
    "outputs = np.array([[56, 70], [81, 101], [119, 133], \n",
    "                    [22, 37], [103, 119], [56, 70], \n",
    "                    [81, 101], [119, 133], [22, 37], \n",
    "                    [103, 119], [56, 70], [81, 101], \n",
    "                    [119, 133], [22, 37], [103, 119]], \n",
    "                   dtype='float32')\n",
    "\n",
    "inputs = torch.from_numpy(inputs)\n",
    "outputs = torch.from_numpy(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5bbc34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In PyTorch, the Dataset and DataLoader are essential components for handling data in the training of machine learning models.\n",
    "\n",
    "# Dataset\n",
    "# A Dataset in PyTorch stores the samples and their corresponding labels. It provides an interface for accessing the \n",
    "# training data. PyTorch supports two types of datasets: map-style and iterable-style datasets. Map-style datasets \n",
    "# implement the __getitem__() and __len__() protocols, while iterable-style datasets represent an iterable over a dataset.\n",
    "    \n",
    "# DataLoader\n",
    "# On the other hand, a DataLoader is an iterable that abstracts the complexity of data handling. It wraps around a \n",
    "# Dataset and enables easy access to the samples, typically in minibatches. The DataLoader supports both map-style \n",
    "# and iterable-style datasets with single- or multi-process loading, allowing for customizing loading order and \n",
    "# optional automatic batching.\n",
    "# The DataLoader is particularly useful for handling large datasets that cannot fit into memory, as well as for \n",
    "# performing data augmentation and preprocessing. It takes care of shuffling, sampling, batching, and using \n",
    "# multiprocessing to load the data, among other functionalities.    \n",
    "\n",
    "# In summary, the Dataset is used to store and provide access to the training data, while the DataLoader is used to \n",
    "# iterate over the dataset in batches, handling various data loading complexities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ad332d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[73., 67., 43.]]), tensor([[56., 70.]]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Load data\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train_ds = TensorDataset(inputs, outputs)\n",
    "\n",
    "# access a small section of the training data using the array indexing notation\n",
    "train_ds[0:1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b4188c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# In each iteration, the data loader returns one batch of data, with the given batch size. If shuffle is set to True, \n",
    "# it shuffles the training data before creating batches. \n",
    "# Shuffling helps randomize the input to the optimization algorithm, \n",
    "# which can lead to faster reduction in the loss.\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size = 5, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f41657ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.3902,  0.1772, -0.5584],\n",
      "        [ 0.4900, -0.2220, -0.4178]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2324, -0.4879], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.3902,  0.1772, -0.5584],\n",
       "         [ 0.4900, -0.2220, -0.4178]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.2324, -0.4879], requires_grad=True)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Define Model\n",
    "\n",
    "model = torch.nn.Linear(3, 2)\n",
    "print(model.weight)\n",
    "print(model.bias)\n",
    "\n",
    "# PyTorch models also have a helpful .parameters method, which returns a list containing all the weights and bias matrices \n",
    "# present in the model.\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d3f5aa28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(56.5607, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "preds\n",
    "\n",
    "# Import nn.functional\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define loss function\n",
    "loss_fn = F.mse_loss\n",
    "\n",
    "loss = loss_fn(model(inputs), outputs)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ea702d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "# Instead of manually manipulating the modelâ€™s weights & biases using gradients, we can use the optimizer optim.SGD. \n",
    "# SGD stands for stochastic gradient descent. It is called stochastic because samples are selected in batches \n",
    "# (often with random shuffling) instead of as a single group.\n",
    "\n",
    "# Define optimizer\n",
    "opt = torch.optim.SGD(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6c8863c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 140.8094\n",
      "Epoch [20/100], Loss: 287.4706\n",
      "Epoch [30/100], Loss: 101.9382\n",
      "Epoch [40/100], Loss: 322.0182\n",
      "Epoch [50/100], Loss: 130.6283\n",
      "Epoch [60/100], Loss: 187.5990\n",
      "Epoch [70/100], Loss: 116.1184\n",
      "Epoch [80/100], Loss: 53.8673\n",
      "Epoch [90/100], Loss: 14.5780\n",
      "Epoch [100/100], Loss: 51.7240\n"
     ]
    }
   ],
   "source": [
    " # Repeat for given number of epochs\n",
    "for epoch in range(100):\n",
    "\n",
    "    # Train with batches of data\n",
    "    for xb,yb in train_dl:\n",
    "\n",
    "        # 1. Generate predictions\n",
    "        pred = model(xb)\n",
    "\n",
    "        # 2. Calculate loss\n",
    "        loss = loss_fn(pred, yb)\n",
    "\n",
    "        # 3. Compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # 4. Update parameters using gradients\n",
    "        opt.step()\n",
    "\n",
    "        # 5. Reset the gradients to zero\n",
    "        opt.zero_grad()\n",
    "\n",
    "    # Print the progress\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, 100, loss.item()))\n",
    "    \n",
    "    # 1. We use the data loader defined earlier to get batches of data for every iteration.\n",
    "    # 2. Instead of updating parameters (weights and biases) manually, we use opt.step to perform the update, and \n",
    "    #    opt.zero_grad to reset the gradients to zero.\n",
    "    # 3. Weâ€™ve also added a log statement which prints the loss from the last batch of data for every 10th epoch, \n",
    "    #    to track the progress of training. loss.item returns the actual value stored in the loss tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e900d4e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 58.1314,  72.1014],\n",
       "        [ 78.0898,  96.5951],\n",
       "        [126.1944, 139.1498],\n",
       "        [ 26.6826,  47.5283],\n",
       "        [ 91.4669, 105.8314],\n",
       "        [ 58.1314,  72.1014],\n",
       "        [ 78.0898,  96.5951],\n",
       "        [126.1944, 139.1498],\n",
       "        [ 26.6826,  47.5283],\n",
       "        [ 91.4669, 105.8314],\n",
       "        [ 58.1314,  72.1014],\n",
       "        [ 78.0898,  96.5951],\n",
       "        [126.1944, 139.1498],\n",
       "        [ 26.6826,  47.5283],\n",
       "        [ 91.4669, 105.8314]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "99c65281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.],\n",
       "        [ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.],\n",
       "        [ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare with targets\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412aee78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
